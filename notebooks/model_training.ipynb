{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc3c2857",
   "metadata": {},
   "source": [
    "# Tomato Grading AI - Model Training\n",
    "\n",
    "This notebook demonstrates the process of training machine learning models for tomato grading based on image features. We'll extract features from tomato images in three categories (ripe, unripe, and reject) and train classification models to distinguish between them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8869a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ad717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to the path to import our feature extraction module\n",
    "sys.path.append('..')\n",
    "from features.feature_extraction import extract_features_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8933e9f5",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b5284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directories\n",
    "dataset_dir = '../dataset'\n",
    "ripe_dir = os.path.join(dataset_dir, 'ripe')\n",
    "unripe_dir = os.path.join(dataset_dir, 'unripe')\n",
    "reject_dir = os.path.join(dataset_dir, 'reject')\n",
    "\n",
    "# Extract features for each category\n",
    "print(\"Extracting features from ripe tomato images...\")\n",
    "ripe_features, ripe_labels = extract_features_batch(ripe_dir, 'ripe')\n",
    "\n",
    "print(\"Extracting features from unripe tomato images...\")\n",
    "unripe_features, unripe_labels = extract_features_batch(unripe_dir, 'unripe')\n",
    "\n",
    "print(\"Extracting features from reject tomato images...\")\n",
    "reject_features, reject_labels = extract_features_batch(reject_dir, 'reject')\n",
    "\n",
    "# Combine all features and labels\n",
    "X = np.vstack((ripe_features, unripe_features, reject_features))\n",
    "y = np.hstack((ripe_labels, unripe_labels, reject_labels))\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of samples per class: Ripe: {len(ripe_features)}, Unripe: {len(unripe_features)}, Reject: {len(reject_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4e2b6a",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98cfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler for later use\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd23f07",
   "metadata": {},
   "source": [
    "## 4. Train SVM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa59d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Train SVM with linear kernel\n",
    "print(\"Training SVM with linear kernel...\")\n",
    "svm_linear = SVC(kernel='linear', probability=True, random_state=42)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search_linear = GridSearchCV(svm_linear, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search_linear.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search_linear.best_score_:.4f}\")\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "print(\"\\nTraining SVM with quadratic kernel...\")\n",
    "svm_quad = SVC(kernel='poly', degree=2, probability=True, random_state=42)\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1]\n",
    "}\n",
    "\n",
    "grid_search_quad = GridSearchCV(svm_quad, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_quad.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search_quad.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search_quad.best_score_:.4f}\")\n",
    "\n",
    "# Save the best models\n",
    "joblib.dump(grid_search_linear.best_estimator_, '../models/svm_linear.pkl')\n",
    "joblib.dump(grid_search_quad.best_estimator_, '../models/svm_quadratic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c745f0e",
   "metadata": {},
   "source": [
    "## 5. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the linear SVM\n",
    "y_pred_linear = grid_search_linear.predict(X_test_scaled)\n",
    "print(\"Linear SVM Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_linear):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_linear))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm_linear = confusion_matrix(y_test, y_pred_linear)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_linear, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['reject', 'ripe', 'unripe'], \n",
    "            yticklabels=['reject', 'ripe', 'unripe'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Linear SVM')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the quadratic SVM\n",
    "y_pred_quad = grid_search_quad.predict(X_test_scaled)\n",
    "print(\"\\nQuadratic SVM Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_quad):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_quad))\n",
    "\n",
    "# Create confusion matrix\n",
    "cm_quad = confusion_matrix(y_test, y_pred_quad)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_quad, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['reject', 'ripe', 'unripe'], \n",
    "            yticklabels=['reject', 'ripe', 'unripe'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Quadratic SVM')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6648b05c",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis\n",
    "\n",
    "Let's train a Random Forest classifier to analyze feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a46c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_count = X.shape[1]\n",
    "color_count = 30\n",
    "texture_count = len(importances) - color_count - 7  # 7 is the number of shape features\n",
    "\n",
    "# Create feature names\n",
    "color_features = [f'Color_{i+1}' for i in range(color_count)]\n",
    "texture_features = [f'Texture_{i+1}' for i in range(texture_count)]\n",
    "shape_features = ['Area', 'Perimeter', 'Circularity', 'Aspect_Ratio', \n",
    "                  'Equiv_Diameter', 'Extent', 'Solidity']\n",
    "\n",
    "feature_names = color_features + texture_features + shape_features\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Group importance by feature type\n",
    "feature_types = ['Color'] * color_count + ['Texture'] * texture_count + ['Shape'] * 7\n",
    "importance_by_type = pd.DataFrame({\n",
    "    'Feature_Type': feature_types,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Calculate sum of importance by type\n",
    "importance_by_type = importance_by_type.groupby('Feature_Type').sum().reset_index()\n",
    "\n",
    "# Plot importance by feature type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Feature_Type', y='Importance', data=importance_by_type)\n",
    "plt.title('Feature Importance by Type')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54200e61",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "1. Extracted color, texture, and shape features from tomato images\n",
    "2. Trained SVM models with linear and quadratic kernels\n",
    "3. Evaluated model performance using accuracy, precision, recall, and F1-score\n",
    "4. Analyzed feature importance using a Random Forest classifier\n",
    "\n",
    "The best model achieved good classification accuracy between ripe, unripe, and reject tomatoes. This model can now be used in the main application for tomato grading."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
